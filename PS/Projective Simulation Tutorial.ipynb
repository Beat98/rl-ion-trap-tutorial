{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projective Simulation Tutorial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a little bit of theory\n",
    "\n",
    "Am not afraid of this. There are just a few things you should understand basically bevore we can start with practice. So let's begin: <br>\n",
    "At first we should understand the model of Reinforcement Learning (RL), because Projective Simulation (PS) is a RL model. So take a look on this illustration:\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/agent_env_interaction.png\" width= \"400\"/>\n",
    "\n",
    "You see the typical interaction between an agent and the environment. In our case the environment is the Ion Trap Environment (see Tutorial). The agent performs an action on the environment according to his observation and receives a possible reward. In the following we want to build a PS agent who is able to maximally entangle $N$ ions. So let's talk a bit about PS: <br>\n",
    "<br>\n",
    "Please do me a favor and read at least the abstract of this paper: __[link](https://arxiv.org/pdf/1104.3787.pdf)__ <br>\n",
    "The following sentence is important: \n",
    ">\"Projective simulation is based on a random walk through a network of clips,\n",
    "which are elementary patches of episodic memory\"\n",
    "\n",
    "The brain of the agent is the so-called episodic compositional memory (ECM), which contains some network of clips. The clips are some instances of memory. Here you see an example:\n",
    "<img src=\"images/general_clips_structure.png\" width=\"300\"/>\n",
    " \n",
    "If the agent makes an observation an so called percept clip (blue) is activated or generated and based on the structure of the clip network and the weights of the so called edges (blue arrows) the agent chooses an action clip (red) and executes an action. This is called random walk. <br>\n",
    "<br>\n",
    "Here we do a so called action encoding. Therefore every percept clip gets its own decision tree and there is no connection between different percept clips.\n",
    "\n",
    "Now that you understand the principal of PS let's start with the implementation! The mathematics will be introduced step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we need something which represents the clip network with the clips, the edges and the weights of the edges. Therefore an adjencency matrix is very usefull. I have an example for you:\n",
    "\n",
    "<img src=\"images/adj_matrix_example.png\" width=\"500\"/>\n",
    "<br>\n",
    "A field from the adjancency matrix represents the weight of a certain edge, which connects two clips. The edge points from one clip (row index) to another clip (column index). The agent is learning by adjusting the weights of the edges. In the beginning all weights are set to one. Later on we will see how the weights are changed.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agents sceleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to build a skeleton of our agent. <br>\n",
    "The agent should have the ability to choose an action (here the id of the action) for a given observation and it should learn from the recieved reward. Therefore it needs an ECM, the actions and the decision tree structure (adjancency matrix with weights set to one). <br>\n",
    "__TASK__: Write a new class skeleton `Agent()` with the `__init__()` function and two methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__: <br>\n",
    "This is a possible solution for an agent skeleton. I have completed the docs for you :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalAgent(object):\n",
    "    def __init__(self, ECM, actions, adj_matrix):\n",
    "        \"\"\"\n",
    "            Projective Simulation Agent. Any typical ECM can be used.\n",
    "\n",
    "            Args:\n",
    "                ECM (object): Episodic compositional memory (ECM). The brain of the agent.\n",
    "                actions (np.ndarray): An array of possible actions. Specified by the environment.\n",
    "                adj_matrix (np.ndarray): Adjancency matrix representing the structure of the default decision tree.\n",
    "            \"\"\"\n",
    "\n",
    "        self.ECM = ECM\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.actions = actions\n",
    "\n",
    "    def step(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation, returns the id of an action clip.\n",
    "\n",
    "        Args:\n",
    "            observation (object): The observation in some form of encoding.\n",
    "        Returns:\n",
    "            actionClip_id (int): The id of the chosen action clip.\n",
    "        \"\"\"\n",
    "\n",
    "        return actionClip_id\n",
    "\n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, updates adjancency matrix and g matrix. See documentation of the ECM for the math.\n",
    "        Args:\n",
    "            reward (float): The received reward.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some functionality for the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to think about the functionality of the agent. Let's look at the `step()` method. Now i will explain, what the agent needs to do there and you will try to implement the expained methods from the ECM afterwards. <br>\n",
    "(1) The agent accesses the ECM in order to either create a new percept clip for the observation or find an existing one, which was created before. If a new one was created it needs the adjancency matrix. \n",
    "(2) The agent performs a random walk through the decision tree of this percept clip and dicides for an action. Retruns the id of this action.\n",
    "\n",
    "Here the completed `step()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation, returns the id of an action clip.\n",
    "\n",
    "        Args:\n",
    "            observation (object): The observation in some form of encoding.\n",
    "        Returns:\n",
    "            actionClip_id (int): The id of the chosen action clip.\n",
    "        \"\"\"\n",
    "\n",
    "        self.activeClip = self.ECM.get_or_create_percept_clip(observation, self.adj_matrix)\n",
    "        actionClip_id = self.ECM.random_walk(self.activeClip)\n",
    "\n",
    "        return actionClip_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `learn()` function is very easy. Here the agent just accesses the learn method from the ECM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, updates adjancency matrix and g matrix. See documentation of the ECM for the math.\n",
    "        Args:\n",
    "            reward (float): The received reward.\n",
    "        \"\"\"\n",
    "        self.ECM.learn(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE agent\n",
    "\n",
    "Nice! We have a full functioning agent. A very silly agent, because the brain (ECM) is not implemented so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalAgent(object):\n",
    "    def __init__(self, ECM, actions, adj_matrix):\n",
    "        \"\"\"\n",
    "            Projective Simulation Agent. Any typical ECM can be used.\n",
    "\n",
    "            Args:\n",
    "                ECM (object): Episodic compositional memory (ECM). The brain of the agent.\n",
    "                actions (np.ndarray): An array of possible actions. Specified by the environment.\n",
    "                adj_matrix (np.ndarray): Adjancency matrix representing the structure of the default decision tree.\n",
    "            \"\"\"\n",
    "\n",
    "        self.ECM = ECM\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.actions = actions\n",
    "\n",
    "    def step(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation, returns the id of an action clip.\n",
    "\n",
    "        Args:\n",
    "            observation (object): The observation in some form of encoding.\n",
    "        Returns:\n",
    "            actionClip_id (int): The id of the chosen action clip.\n",
    "        \"\"\"\n",
    "\n",
    "        self.activeClip = self.ECM.get_or_create_percept_clip(observation, self.adj_matrix)\n",
    "        actionClip_id = self.ECM.random_walk(self.activeClip)\n",
    "\n",
    "        return actionClip_id\n",
    "\n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, updates adjancency matrix and g matrix.\n",
    "        Args:\n",
    "            reward (float): The received reward.\n",
    "        \"\"\"\n",
    "        self.ECM.learn(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The brain of the agent (ECM)\n",
    "\n",
    "In the ECM the percept and action clips are created and stored.  We use a dictionary with following form:\n",
    "> percept_dict: {percept1 (str) : percept_clip1 (object), percept2 (str) : percept_clip2 (object), ...} <br>\n",
    "> action_dict: {action1 (str) : action_clip1 (object), action2 (str) : action_clip2 (object),...}\n",
    "\n",
    "We see two things: First the percept is stored as string, so we have to preprosses it somewhere. Secondly the percept and action clips are objects, so we need classes for the clips. <br>\n",
    "__TASK__:  <br>\n",
    "- Implement a parent class `Clip()` with instance attributes `self.id` and `self.type = \"Clip\"`\n",
    "- Implement the classes `ActionClip(Clip)` and `PerceptClip(Clip)`. They should inherit the id from the `Clip()` class.\n",
    "- Each clip needs an id and a label and the type. The label is just the percept (str) or the action (str). A percept clip needs also the adjancency matrix and a g-matrix (explained later). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clip():\n",
    "    \"\"\"a clip\"\"\"\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.type = \"Clip\"\n",
    "\n",
    "\n",
    "class ActionClip(Clip):\n",
    "    \"\"\"an action clip\"\"\"\n",
    "    def __init__(self, id, action):\n",
    "        Clip.__init__(self, id)\n",
    "        self.label = action\n",
    "        self.type = \"ActionClip\"\n",
    "\n",
    "\n",
    "class PerceptClip(Clip):\n",
    "    \"\"\"a percept clip\"\"\"\n",
    "    def __init__(self, id, percept, adj_matrix, g_matrix):\n",
    "        Clip.__init__(self, id)\n",
    "        self.label = percept\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.g_matrix = g_matrix\n",
    "        self.type = \"PerceptClip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take a look in the step and learn function of the agent. You probably see, that we should implement three functions from the ECM: <br>\n",
    "- ECM.get_or_create_percept_clip()\n",
    "- ECM.random_walk()\n",
    "- ECM.learn()\n",
    "\n",
    "Now i will provide you the ECM skeleton, but it is your job to give it life later on. You will find the three functions in the code. <br>\n",
    "__TASK__: Read the docs carefully to get an overview of the ECM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ecm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bd0c2dc9d561>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mecm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClips\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mActionClip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPerceptClip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mUniversalECM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ecm'"
     ]
    }
   ],
   "source": [
    "### do not run ###\n",
    "\n",
    "from ecm.Clips import ActionClip, PerceptClip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class UniversalECM(object):\n",
    "    def __init__(self, gamma_damping=0., eta_glow_damping=0.1, beta=1):\n",
    "    \n",
    "        self.num_actions = 0\n",
    "        self.num_percepts = 0\n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.beta = beta\n",
    "\n",
    "        self.percept_dict = {}  # stores the percept_clips with the corresponding percept, here: {percept (str) : Percept_clip (object), ...}\n",
    "        self.action_dict = {}  # stores the action_clips with the corresponding number, here: {action (str) : Action_clip (object), ...}\n",
    "        \n",
    "\n",
    "    def get_action_dict(self, actions):\n",
    "        \"\"\"\n",
    "        Initializes the action dictionary.\n",
    "        It has this form: {action1 (str) : Action_clip1 (object), action2 (str) : Action_clip2 (object),...}\n",
    "\n",
    "        Args:\n",
    "            actions (array): [action1 (str), action2 (str), ...]\n",
    "        \"\"\"\n",
    "\n",
    "    def get_or_create_percept_clip(self, observation, adj_matrix):\n",
    "        \"\"\"\n",
    "        Finds the corresponding percept clip to the observation or creates a new percept clip for the observation.\n",
    "        (1) Preprocesses the observation to a percept.\n",
    "        (2) Tries to find the corresponding percept clip to this percept; returns the clip.\n",
    "        (3) Creates a new percept clip if the percept has not been encountered before.\n",
    "        (4) Sets the decision tree (in form of the adjancency matrix) and the g matrix as attributes of the percept clip.\n",
    "\n",
    "        Args:\n",
    "            observation (object): The observation in some form of encoding.\n",
    "            adj_matrix (np.ndarray): The adjancency matrix representing the decision tree\n",
    "        Returns:\n",
    "            percept_clip (object): percept clip\n",
    "        \"\"\"\n",
    "\n",
    "    def random_step(self, from_clip_index, adj_matrix, g_matrix):\n",
    "        \"\"\"\n",
    "        Does a random step (transition) from a clip (from_clip_index) to a connected clip (to_clip_index).\n",
    "        The connected clips are weighted with the softmax distribution.\n",
    "\n",
    "        Args:\n",
    "            from_clip_index (int): from this clip a random step will be performed\n",
    "            adj_matrix (np.ndarray): the adjancency matrix of the percept clip from which the random walk is performed\n",
    "            g_matrix (np.ndarray): the  g matrix of the percept clip from which the random walk is performed\n",
    "        Returns:\n",
    "            to_clip_index (int): probabilistically picked clip\n",
    "            finished (boolean): True, if na action clip is found\n",
    "        \"\"\"\n",
    "\n",
    "        return to_clip_index, finished\n",
    "\n",
    "    def random_walk(self, percept_clip):\n",
    "        \"\"\"\n",
    "        Performs a random walk through the decision tree from the percept clip\n",
    "        (encoded in the adjancency matrix) to an action clip. Returns the id (number) of this action clip.\n",
    "\n",
    "        Args:\n",
    "            percept_clip (object): percept clip\n",
    "        Returns:\n",
    "            action_clip_index (int): Id (number) of the found action clip\n",
    "        \"\"\"\n",
    "\n",
    "        return action_clip_index\n",
    "\n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Updates the h_values of all edges according to the reward.\n",
    "        Sets all g values to zero.\n",
    "\n",
    "        Args:\n",
    "            reward (float): received reward\n",
    "        \"\"\"\n",
    "                \n",
    "                \n",
    "### helper functions ###\n",
    "    \n",
    "    def softmax(self, beta, h_values):\n",
    "        \"\"\"\n",
    "        Calculates probabilities according to the softmax distribution.\n",
    "\n",
    "        Args:\n",
    "            beta (float): softmax parameter beta\n",
    "            h_values (np.ndarray): array of the h-values\n",
    "        Returns:\n",
    "            tuple of probabilities, same order as the h_values\n",
    "        \"\"\"\n",
    "        return ex / sum(ex)\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation, returns a percept.\n",
    "        This function is just to emphasize the difference between observations\n",
    "        issued by the environment and percepts which describe the observations\n",
    "        as perceived by the agent.\n",
    "\n",
    "        Args:\n",
    "            observation (object): The observation in some form of encoding.\n",
    "        Returns:\n",
    "            percept (str): The observation encoded as a percept.\n",
    "        \"\"\"\n",
    "        percept = str(observation)\n",
    "        return percept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the three important functions, we should take a look on the `__init__()` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, gamma_damping=0., eta_glow_damping=0.1, beta=1):\n",
    "    \n",
    "    self.num_actions = 0\n",
    "    self.num_percepts = 0\n",
    "    self.gamma_damping = gamma_damping\n",
    "    self.eta_glow_damping = eta_glow_damping\n",
    "    self.beta = beta\n",
    "\n",
    "    self.percept_dict = {}  # stores the percept_clips with the corresponding percept, here: {percept (str) : Percept_clip (object), ...}\n",
    "    self.action_dict = {}  # stores the action_clips with the corresponding number, here: {action (str) : Action_clip (object), ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `num_actions` and `num_percepts` stands for the number of the action or percept clips. Later we use it to give the clips an id. gamma_damping, eta_glow_damping and beta are parameteres you can later play with to get good results. I will explain them when we come to the learning function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECM.get_or_create_percept_clip()\n",
    "\n",
    "Here the percept clips are created and stored. Every percept clip gets its own decision tree (adj_matrix) a so called glow matrix (g_matrix). You can think of it as the short time memory of the agent. I will explain this later in detail. <br>\n",
    "Here are the docs again: <br>\n",
    "> Finds the corresponding percept clip to the observation or creates a new percept clip for the observation. <br>\n",
    "  (1) Preprocesses the observation to a percept. <br>\n",
    "  (2) Tries to find the corresponding percept clip to this percept; returns the clip. <br>\n",
    "  (3) Creates a new percept clip if the percept has not been encountered before. Sets the decision tree (in form of the adjancency matrix) and the g matrix as attributes of the percept clip. <br>\n",
    "<br>\n",
    "\n",
    "__TASK__: <br>\n",
    "I will give you some explanation for the 3 points, and then it is your job to write the code. <br>\n",
    "(1): use the helperfunction `preprocess()`<br>\n",
    "(2): look at the form of the `percept_dict` where the clips are stored and write a condition.<br>\n",
    "(3): initialize an matrix with just zeros with the size of the adj_matrix and call it g_matrix. Store the new clip with `id`, `label`, `adj_matrix` and `g_matrix` in the dictionary. Consider, that the you have to add one to the `num_percepts`, because the next percept clips has to get another id. <br>\n",
    "<br>\n",
    "__SOLUTION__: <br>\n",
    "Your solution will probably differ from this in some way. Neverless it could be also correct. For later i recommend using my solution, because of compatibility reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_percept_clip(self, observation, adj_matrix):\n",
    "    \"\"\"\n",
    "    Finds the corresponding percept clip to the observation or creates a new percept clip for the observation.\n",
    "    (1) Preprocesses the observation to a percept.\n",
    "    (2) Tries to find the corresponding percept clip to this percept; returns the clip.\n",
    "    (3) Creates a new percept clip if the percept has not been encountered before. \n",
    "        Sets the decision tree (in form of the adjancency matrix) and the g matrix as attributes of the percept clip.\n",
    "\n",
    "    Args:\n",
    "        observation (object): The observation in some form of encoding.\n",
    "        adj_matrix (np.ndarray): The adjancency matrix representing the decision tree\n",
    "    Returns:\n",
    "        percept_clip (object): percept clip\n",
    "    \"\"\"\n",
    "\n",
    "    percept = self.preprocess(observation)\n",
    "\n",
    "    if percept in self.percept_dict.keys():\n",
    "        percept_clip = self.percept_dict[percept]\n",
    "        return percept_clip\n",
    "    else:\n",
    "        # initialize g matrix with empty np.array\n",
    "        g_matrix = np.zeros((len(adj_matrix), len(adj_matrix)))\n",
    "\n",
    "        percept_clip = PerceptClip(self.num_percepts, percept, adj_matrix, g_matrix)\n",
    "        self.percept_dict[percept] = percept_clip\n",
    "        self.num_percepts += 1\n",
    "\n",
    "        return percept_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECM.random_walk()\n",
    "\n",
    "Finally we came to a point where we can't ignore the math anymore. As previously explained, the agent chooses an action with a random walk through the decision tree. Each time the agent has to decide between two or more clips, it does a probabilistic choice due to the weights on the edges of the clips. The probabilities are calculated according to the so-called softmax distribution: <br>\n",
    "<br>\n",
    "$p_{ij}^{(t)} = p^{(t)}(c_i \\rightarrow c_j) = e^{\\beta \\tilde{h}_{ij}^{(t)}} / \\sum_k e^{\\beta \\tilde{h}_{ik}^{(t)}}$ <br>\n",
    "<br>\n",
    "with<br>\n",
    "<br>\n",
    "$\\tilde{h}_{ij}^{(t)} = h_{ij}^{(t)} - \\max_{k \\in \\mathcal{N}_i} h_{ik}^{(t)}$<br>\n",
    "<br>\n",
    "$p_{ij}^{(t)}$ is the probabilitiy at time $(t)$, that the agent chooses the transition from clip $i$ to clip $j$. $h_{ij}$ is a so called h value (weight), which is the $ij$-th entry of the adjancency matrix. The second equation avoids computational errors due to large numbers and $\\mathcal{N}_i$ is the set of all clips connected to clip $c_i$. <br>\n",
    "$\\beta$ is the so-called softmax parameter. Later you can play with this paramter to see its effects on the results. I don't want to spoil you now :)\n",
    "<br> \n",
    "__TASK:__ <br>\n",
    "Implement the math for the `softmax()` function according to the equations above and the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self, beta, h_values):\n",
    "        \"\"\"\n",
    "        Calculates probabilities according to the softmax distribution.\n",
    "\n",
    "        Args:\n",
    "            beta (float): softmax parameter beta\n",
    "            h_values (np.ndarray): array of h-values (row of the adjancency matrix without zero entries)\n",
    "        Returns:\n",
    "            prob (np.ndarray): array of probabilities, same order as the h_values\n",
    "        \"\"\"\n",
    "        # fill in\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self, beta, h_values):\n",
    "        \"\"\"\n",
    "        Calculates probabilities according to the softmax distribution.\n",
    "\n",
    "        Args:\n",
    "            beta (float): softmax parameter beta\n",
    "            h_values (np.ndarray): array of h-values (row of the adjancency matrix without zero entries)\n",
    "        Returns:\n",
    "            tuple of probabilities, same order as the h_values\n",
    "        \"\"\"\n",
    "        ex = np.array(h_values, dtype=np.float64) * beta\n",
    "        ex = ex - max(ex)\n",
    "        ex = np.exp(ex)\n",
    "        prob = ex / sum(ex)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The first important step for the random walk is done. Out of an array of weigths we can get the probablities. The next goal is to write a function which is able to perform a random step. The first random step is the the step from the percept clip to one of the connected ones. We now try to write a funktion which takes the index of one clip and returns the index of the choosen clip according to the weights. The funktion should also have an output indicating if an action clip is found. For the warm up i have a short task for you: <br>\n",
    "__TASK:__ <br>\n",
    "Write down any arbitrary adjanceny matrix (you can also use the one from above) and find some easy way to see if a clip is an action clip just from the entries of the matrix. <br>\n",
    "<br>\n",
    "Now we implement the whole function. Therefore i have to explain one more thing. You may asked yourself what the g matrix does exactly. The g matrix is something like a short time memory of the agent. Previously made steps are stored in this matrix. At the beginning this matrix is just filled with zeros and has the same size as the adjancency matrix. If a random step is done this step is stored in the g matrix. The corresponding entry for the chosen edge in the g matrix is set to one. <br>\n",
    "Now its your turn! Following functions could be useful: `enumerate()`and `np.random.choice()`<br>\n",
    "__TASK:__ <br>\n",
    "Implement the `random_step()` with the help of the explanation and the docs. This task is a harder then the previous ones. But please try it at first by youself. It's no shame if you need the solution for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_step(self, from_clip_index, adj_matrix, g_matrix):\n",
    "    \"\"\"\n",
    "    Does a random step (transition) from a clip (from_clip_index) to a connected clip (to_clip_index).\n",
    "    The connected clips are weighted with the softmax distribution.\n",
    "\n",
    "    Args:\n",
    "        from_clip_index (int): from this clip a random step will be performed\n",
    "        adj_matrix (np.ndarray): the adjancency matrix of the percept clip from which the random walk is performed\n",
    "        g_matrix (np.ndarray): the  g matrix of the percept clip from which the random walk is performed\n",
    "    Returns:\n",
    "        to_clip_index (int): probabilistically picked clip\n",
    "        finished (boolean): True, if an action clip is found\n",
    "    \"\"\"\n",
    "    return to_clip_index, finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def random_step(self, from_clip_index, adj_matrix, g_matrix):\n",
    "        \"\"\"\n",
    "        Does a random step (transition) from a clip (from_clip_index) to a connected clip (to_clip_index).\n",
    "        The connected clips are weighted with the softmax distribution.\n",
    "\n",
    "        Args:\n",
    "            from_clip_index (int): from this clip a random step will be performed\n",
    "            adj_matrix (np.ndarray): the adjancency matrix of the percept clip from which the random walk is performed\n",
    "            g_matrix (np.ndarray): the  g matrix of the percept clip from which the random walk is performed\n",
    "        Returns:\n",
    "            to_clip_index (int): probabilistically picked clip\n",
    "            finished (boolean): True, if an action clip is found\n",
    "        \"\"\"\n",
    "\n",
    "        finished = False\n",
    "\n",
    "        row = adj_matrix[from_clip_index]\n",
    "        enum_row = list(enumerate(row))\n",
    "        \n",
    "        # filter out the zero entries as there is no edge\n",
    "        filtered_row = [enum_row[i] for i in range(len(enum_row)) if enum_row[i][1] != 0]\n",
    "\n",
    "        if len(filtered_row) != 0:  # if the row has now entry != 0 an action clip is found\n",
    "\n",
    "            # connected_clips_indices: indices of the connected clips\n",
    "            # connected_clips_probabilities: h values of the connected clips\n",
    "            connected_clips_indices = [filtered_row[i][0] for i in range(len(filtered_row))]\n",
    "            connected_clips_probabilities = np.array([filtered_row[i][1] for i in range(len(filtered_row))])\n",
    "\n",
    "            # pick one clip of all connected clips randomly weighted with the softmax distribution\n",
    "            prob = self.softmax(self.beta, connected_clips_probabilities\n",
    "            to_clip_index = np.random.choice(connected_clips_indices, p=prob))\n",
    "\n",
    "\n",
    "            # set g value of the clip transition to one\n",
    "            x, y = from_clip_index, to_clip_index\n",
    "            g_matrix[x, y] = 1\n",
    "\n",
    "        else:\n",
    "            to_clip_index = from_clip_index\n",
    "            finished = True\n",
    "\n",
    "        return to_clip_index, finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! The hard part is over. But the agent is still not able to do a random walk and to learn. So let's finally introduce the most important equations of Projective Simulation: <br>\n",
    "<br>\n",
    "$h_{ij}^{'} = h_{ij} - \\gamma \\cdot (h_{ij} -1) + g_{ij} \\cdot \\lambda$<br>\n",
    "<br>\n",
    "and\n",
    "<br>\n",
    "<br>\n",
    "$g_{ij}^{'} = g_{ij} \\cdot (1- \\eta)$<br>\n",
    "<br>\n",
    "\n",
    "These equations determine how the h and g values are updated. The whole network is updated according to these equations every time a random step was made. $\\gamma$ is the gamma damping parameter. Here we can set it to zero, because it is just important if the environment changes in time. The reward $\\lambda$ is one if the agent found the maximal SRV (see Tutorial), zero otherwise. $\\eta$ is the glow damping parameter. This parameter is important for problems with a delayed reward. That means that the agent does not recieve a reward after each step (f.e. gridworld environment). It needs to remember the sequence of steps which brougt it to the reward. With $\\eta$ one can adjust the shorttime forgetfulness of the agent. <br>\n",
    "Note, that the first equation to update the h values is just needed in the `learn()` function, because if the agent does not learn (the reward is zero) all h values stay the same.<br>\n",
    "Here it is better to apply the second equation not after each random step, but after each random walk. So let's do this! I will give you the solution straight, because you are still here and for that i want to do you a favor :)<br>\n",
    "Despite of that, please read the function carfully and try to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def random_walk(self, percept_clip):\n",
    "        \"\"\"\n",
    "        Performs a random walk through the decision tree from the percept clip\n",
    "        (encoded in the adjancency matrix) to an action clip. Returns the id (number) of this action clip.\n",
    "\n",
    "        Args:\n",
    "            percept_clip (object): percept clip\n",
    "        Returns:\n",
    "            action_clip_index (int): Id (number) of the found action clip\n",
    "        \"\"\"\n",
    "\n",
    "        # check, if the input clip is a percept clip\n",
    "        if percept_clip.type != \"PerceptClip\":\n",
    "            print(\"first clip in random walk needs to be a percept_clip\")\n",
    "\n",
    "        # adjancency matrix g_matrix (np.ndarray) from the percept clip\n",
    "        adj_matrix = percept_clip.adj_matrix\n",
    "        g_matrix = percept_clip.g_matrix\n",
    "\n",
    "        # all clips connected to a percept clip are in the first row\n",
    "        from_clip_index = 0\n",
    "\n",
    "        finished = False\n",
    "\n",
    "        # do random steps until reaching a action clip\n",
    "        while not finished:\n",
    "            to_clip_index, finished = self.random_step(from_clip_index, adj_matrix, g_matrix)\n",
    "            from_clip_index = to_clip_index\n",
    "\n",
    "        # damp all g values from the whole network\n",
    "        for clip in self.percept_dict.values():\n",
    "            if clip != percept_clip:\n",
    "                clip.g_matrix *= (1 - self.eta_glow_damping)\n",
    "\n",
    "        # subtract one to get the action clip index, because the first clip is always the percept clip\n",
    "        action_clip_index = to_clip_index - 1\n",
    "\n",
    "        return action_clip_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECM.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last function we have to implement. The whole network should be updated according to the first equation explained above. <br>\n",
    "__TASK:__ <br>\n",
    "Fill in the code according to the explanation and the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Updates the h_values of all edges according to the reward.\n",
    "        Sets all g values to zero.\n",
    "\n",
    "        Args:\n",
    "            reward (float): received rewardThis\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION:__ <br>\n",
    "I added the last two rows. Here the g matrix is set to zero again if the agent learnt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Updates the h_values of all edges according to the reward.\n",
    "        Sets all g values to zero.\n",
    "\n",
    "        Args:\n",
    "            reward (float): received reward\n",
    "        \"\"\"\n",
    "        if reward != 0.0:\n",
    "            for clip in self.percept_dict.values():\n",
    "                clip.adj_matrix = clip.adj_matrix + reward * clip.g_matrix\n",
    "\n",
    "            for clip in self.percept_dict.values():\n",
    "                clip.g_matrix *= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE Agent\n",
    "\n",
    "Congratulations! You made it. You have written a fully functioning PS Agent! Here the final code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class UniversalECM(object):\n",
    "    def __init__(self, gamma_damping=0., eta_glow_damping=0.1, beta=1):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_actions = 0\n",
    "        self.num_percepts = 0\n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.beta = beta\n",
    "\n",
    "        self.percept_dict = {}  # stores the percept_clips with the corresponding percept, here: {percept (str) : Percept_clip (object), ...}\n",
    "        self.action_dict = {}  # stores the action_clips with the corresponding number, here: {action (str) : Action_clip (object), ...}\n",
    "\n",
    "    def get_action_dict(self, actions):\n",
    "        \"\"\"\n",
    "        Initializes the action dictionary.\n",
    "        It has this form: {action1 (str) : Action_clip1 (object), action2 (str) : Action_clip2 (object),...}\n",
    "\n",
    "        Args:\n",
    "            actions (array): [action1 (str), action2 (str), ...]\n",
    "        \"\"\"\n",
    "        for action in actions:\n",
    "            action_clip = ActionClip(self.num_actions, action)\n",
    "            self.action_dict[action] = action_clip\n",
    "            self.num_actions += 1\n",
    "\n",
    "    def get_or_create_percept_clip(self, observation, adj_matrix):\n",
    "        \"\"\"\n",
    "        Finds the corresponding percept clip to the observation or creates a new percept clip for the observation.\n",
    "        (1) Preprocesses the observation to a percept.\n",
    "        (2) Tries to find the corresponding percept clip to this percept; returns the clip.\n",
    "        (3) Creates a new percept clip if the percept has not been encountered before.\n",
    "        (4) Sets the decision tree (in form of the adjancency matrix) and the g matrix as attributes of the percept clip.\n",
    "\n",
    "        Args:\n",
    "            observation (object): The observation in some form of encoding.\n",
    "            adj_matrix (np.ndarray): The adjancency matrix representing the decision tree\n",
    "        Returns:\n",
    "            percept_clip (object): percept clip\n",
    "        \"\"\"\n",
    "\n",
    "        percept = self.preprocess(observation)\n",
    "\n",
    "        if percept in self.percept_dict.keys():\n",
    "            percept_clip = self.percept_dict[percept]\n",
    "            return percept_clip\n",
    "        else:\n",
    "            # initialize g matrix with empty np.array\n",
    "            g_matrix = np.zeros((len(adj_matrix), len(adj_matrix)))\n",
    "\n",
    "            percept_clip = PerceptClip(self.num_percepts, percept, adj_matrix, g_matrix)\n",
    "            self.percept_dict[percept] = percept_clip\n",
    "            self.num_percepts += 1\n",
    "\n",
    "            return percept_clip\n",
    "\n",
    "    def get_percept_clip(self, percept):\n",
    "        return self.percept_dict[percept]\n",
    "\n",
    "    def get_action_clip(self, action):\n",
    "        return self.action_dict[action]\n",
    "\n",
    "    def random_step(self, from_clip_index, adj_matrix, g_matrix):\n",
    "        \"\"\"\n",
    "        Does a random step (transition) from a clip (from_clip_index) to a connected clip (to_clip_index).\n",
    "        The connected clips are weighted with the softmax distribution.\n",
    "\n",
    "        Args:\n",
    "            from_clip_index (int): from this clip a random step will be performed\n",
    "            adj_matrix (np.ndarray): the adjancency matrix of the percept clip from which the random walk is performed\n",
    "            g_matrix (np.ndarray): the  g matrix of the percept clip from which the random walk is performed\n",
    "        Returns:\n",
    "            to_clip_index (int): probabilistically picked clip\n",
    "            finished (boolean): True, if an action clip is found\n",
    "        \"\"\"\n",
    "\n",
    "        finished = False\n",
    "\n",
    "        row = adj_matrix[from_clip_index]\n",
    "        enum_row = list(enumerate(row))\n",
    "        filtered_row = [enum_row[i] for i in range(len(enum_row)) if enum_row[i][1] != 0]\n",
    "\n",
    "        if len(filtered_row) != 0:  # if the row has now entry != 0 an action clip is found\n",
    "\n",
    "            # connected_clips_indices: indices of the connected clips\n",
    "            # connected_clips_probabilities: h values of the connected clips\n",
    "            connected_clips_indices = [filtered_row[i][0] for i in range(len(filtered_row))]\n",
    "            connected_clips_probabilities = np.array([filtered_row[i][1] for i in range(len(filtered_row))])\n",
    "\n",
    "            # pick one clip of all connected clips randomly weighted with the softmax distribution\n",
    "            to_clip_index = np.random.choice(connected_clips_indices,\n",
    "                                             p=self.softmax(self.beta, connected_clips_probabilities))\n",
    "\n",
    "            # set g value of the clip transition to one\n",
    "            x, y = from_clip_index, to_clip_index\n",
    "            g_matrix[x, y] = 1\n",
    "\n",
    "        else:\n",
    "            to_clip_index = from_clip_index\n",
    "            finished = True\n",
    "\n",
    "        return to_clip_index, finished\n",
    "\n",
    "    def random_walk(self, percept_clip):\n",
    "        \"\"\"\n",
    "        Performs a random walk through the decision tree from the percept clip\n",
    "        (encoded in the adjancency matrix) to an action clip. Returns the id (number) of this action clip.\n",
    "\n",
    "        Args:\n",
    "            percept_clip (object): percept clip\n",
    "        Returns:\n",
    "            action_clip_index (int): Id (number) of the found action clip\n",
    "        \"\"\"\n",
    "\n",
    "        # check, if the input clip is a percept clip\n",
    "        if percept_clip.type != \"PerceptClip\":\n",
    "            print(\"first clip in random walk needs to be a percept_clip\")\n",
    "\n",
    "        # adjancency matrix g_matrix (np.ndarray) from the percept clip\n",
    "        adj_matrix = percept_clip.adj_matrix\n",
    "        g_matrix = percept_clip.g_matrix\n",
    "\n",
    "        # all clips connected to a percept clip are in the first row\n",
    "        from_clip_index = 0\n",
    "\n",
    "        finished = False\n",
    "\n",
    "        # do random steps until reaching a action clip\n",
    "        while not finished:\n",
    "            to_clip_index, finished = self.random_step(from_clip_index, adj_matrix, g_matrix)\n",
    "            from_clip_index = to_clip_index\n",
    "\n",
    "        # damp all g values from the whole network\n",
    "        for clip in self.percept_dict.values():\n",
    "            if clip != percept_clip:\n",
    "                clip.g_matrix *= (1 - self.eta_glow_damping)\n",
    "\n",
    "        # subtract one to get the action clip index, because the first clip is always the percept clip\n",
    "        action_clip_index = to_clip_index - 1\n",
    "\n",
    "        return action_clip_index\n",
    "\n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Updates the h_values of all edges according to the reward.\n",
    "        Sets all g values to zero.\n",
    "\n",
    "        Args:\n",
    "            reward (float): received reward\n",
    "        \"\"\"\n",
    "        if reward != 0.0:\n",
    "            for clip in self.percept_dict.values():\n",
    "                clip.adj_matrix = clip.adj_matrix + reward * clip.g_matrix\n",
    "\n",
    "            for clip in self.percept_dict.values():\n",
    "                clip.g_matrix *= 0\n",
    "                \n",
    "    #### helper functions ####\n",
    "\n",
    "    def softmax(self, beta, h_values):\n",
    "        \"\"\"\n",
    "        Calculates probabilities according to the softmax distribution.\n",
    "\n",
    "        Args:\n",
    "            beta (float): softmax parameter beta\n",
    "            h_values (np.ndarray): array of the h-values\n",
    "        Returns:\n",
    "            tuple of probabilities, same order as the h_values\n",
    "        \"\"\"\n",
    "        ex = np.array(h_values, dtype=np.float64) * beta\n",
    "        ex = ex - max(ex)\n",
    "        ex = np.exp(ex)\n",
    "\n",
    "        return ex / sum(ex)\n",
    "\n",
    "    def preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation, returns a percept.\n",
    "        This function is just to emphasize the difference between observations\n",
    "        issued by the environment and percepts which describe the observations\n",
    "        as perceived by the agent.\n",
    "\n",
    "        Args:\n",
    "            observation (object): The observation in some form of encoding.\n",
    "        Returns:\n",
    "            percept (str): The observation encoded as a percept.\n",
    "        \"\"\"\n",
    "        percept = str(observation)\n",
    "        return percept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that this code is the most basic version of PS. You can give a tree structure as input, but it can't change. You could improve the code by adjusting the tree structure during the process of learning to generalize over actions for example. <br>\n",
    "If you are further interested in the theory of PS i recommend to read the following paper: <br>\n",
    "- __[Projective simulation for artificial intelligence](https://arxiv.org/pdf/1104.3787.pdf)__\n",
    "- __[Projective simulation with generalization](https://arxiv.org/pdf/1504.02247.pdf)__\n",
    "\n",
    "\n",
    "Now it is time to play with the agent and the ion trap environment. You can try this by yourself or you do the third tutorial. See you there :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
